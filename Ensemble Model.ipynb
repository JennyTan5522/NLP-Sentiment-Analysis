{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54a963b",
   "metadata": {},
   "source": [
    "# Ensemble Model\n",
    "\n",
    "The Ensemble Model used is a combination of Recurrent Neural Network (RNN), Long-Short-Term-Memory Network (LSTM) and Gated Recurrent Unit (GRU) to form a Stacked RNN-LSTM-GRU ensemble model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560b22e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "Libraries needed for are imported in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62c54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import keras\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5409180",
   "metadata": {},
   "source": [
    "## 2. Read Training Dataset\n",
    "The training dataset used is the dataset downloaded from Kaggle that contains around 27k rows of Tweets with their sentiments labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83507fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: \n",
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment Time of Tweet Age of User  \\\n",
      "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
      "1                             Sooo SAD  negative          noon       21-30   \n",
      "2                          bullying me  negative         night       31-45   \n",
      "3                       leave me alone  negative       morning       46-60   \n",
      "4                        Sons of ****,  negative          noon       60-70   \n",
      "\n",
      "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
      "0  Afghanistan          38928346         652860.0               60  \n",
      "1      Albania           2877797          27400.0              105  \n",
      "2      Algeria          43851044        2381740.0               18  \n",
      "3      Andorra             77265            470.0              164  \n",
      "4       Angola          32866272        1246700.0               26  \n"
     ]
    }
   ],
   "source": [
    "# read training dataset\n",
    "train = pd.read_csv(\"train.csv\", encoding = \"unicode_escape\")\n",
    "print(\"Initial: \")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699b376",
   "metadata": {},
   "source": [
    "## 3. Drop Rows with Missing Values\n",
    "Rows that contain missing values are dropped to clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d047d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with null values\n",
    "train = train.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7603bf",
   "metadata": {},
   "source": [
    "## 4. Preprocess Texts\n",
    "The steps used to preprocess texts are as follows: <br>\n",
    "a. Remove Links <br>\n",
    "b. Remove Remove Emails <br>\n",
    "c. Remove New Line Characters <br>\n",
    "d. Remove Numbers <br>\n",
    "e. Remove Emojis <br>\n",
    "f. Remove Punctuations and Accents <br>\n",
    "g. Remove Irrelevant Words (Length shorter or equal to 2 OR length greater or equal to 15) <br>\n",
    "h. Remove Stopwords <br>\n",
    "i. Tokenization <br>\n",
    "j. Part-of-Speech (POS) Tagging <br>\n",
    "k. Lemmatization <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fe8039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['respond go', 'sooo sad miss san diego', 'bos bully', 'interview leave alone', 'son put release already buy']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def get_wordnet_pos(treebank_pos):\n",
    "    if treebank_pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def depure_data(data):\n",
    "    # Remove URL\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    data = url_pattern.sub(r'', data)\n",
    "\n",
    "    # Remove Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "    \n",
    "    # Remove distracting single quotes\n",
    "    data = re.sub(\"\\'\", \"\", data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # 1. Converts to lowercase\n",
    "        # 2. Removes accents and punctuations\n",
    "        # 3. Removes words shorter (<=) or longer (>=) than minimum length of 2 and maximum length of 15\n",
    "        # 4. Removes numerical characters\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)\n",
    "\n",
    "def stopwords_lemmatize(data):\n",
    "    # Remove stopwords\n",
    "    data = \" \".join([word for word in str(data).split() if word not in stop_words])\n",
    "    \n",
    "    # Apply tokenization\n",
    "    data = nltk.word_tokenize(data)\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    data = nltk.pos_tag(data)\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        word, pos = data[i]\n",
    "        \n",
    "        lemmatized_tokens.append(nltk.WordNetLemmatizer().lemmatize(word, get_wordnet_pos(pos)))\n",
    "    \n",
    "    data = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return data\n",
    "\n",
    "temp = []\n",
    "# Convert the values in the dataset to a list\n",
    "data_to_list = train['text'].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314adce4",
   "metadata": {},
   "source": [
    "## 5. Label Encoding\n",
    "The label is encoded into 1 for positive, 0 for neutral and -1 for negative before it is one-hot encoded into the following structure: <br>\n",
    "[neutral, positive, negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c3c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "train['sentiment'] = train['sentiment'].map({\"positive\": 1, \"neutral\": 0, \"negative\": -1})\n",
    "y = train['sentiment'].to_numpy()\n",
    "y = keras.utils.to_categorical(y, 3)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12a7c76",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction\n",
    "The feature vector or raw word vector is extracted by converting the texts into sequence of numbers. <br>\n",
    "The maximum number of words that will be stored in the tokenizer is 5000 and the maximum length of the vector is 200, texts that are shorter than the maximum length will be padded with 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161210f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0 1208    2]\n",
      " [   0    0    0 ...   19 1209 1898]\n",
      " [   0    0    0 ...    0 1162 3767]\n",
      " ...\n",
      " [   0    0    0 ...  434  668 2348]\n",
      " [   0    0    0 ...    0    0  523]\n",
      " [   0    0    0 ...  460  143  312]]\n"
     ]
    }
   ],
   "source": [
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685dda5e",
   "metadata": {},
   "source": [
    "## 7. Train Validation Split\n",
    "The training dataset is split by 80% and 20% into train and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba4b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb6b5c",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "The models in this file are the three individual models, RNN, LSTM, GRU and the ensemble model, Stacked RNN-LSTM-GRU ensemble model. After each model is trained, the model is saved to a .h5 file so that the model can be loaded for prediction without training again and for training the meta-learner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4f0d0",
   "metadata": {},
   "source": [
    "### a. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81da6a1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 200, 40)           200000    \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 125)               20750     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 125)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 10)                1260      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,043\n",
      "Trainable params: 222,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 1.0999 - accuracy: 0.3852\n",
      "Epoch 1: val_accuracy improved from -inf to 0.41303, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 14s 37ms/step - loss: 1.0999 - accuracy: 0.3852 - val_loss: 1.0919 - val_accuracy: 0.4130\n",
      "Epoch 2/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 1.0935 - accuracy: 0.3982\n",
      "Epoch 2: val_accuracy improved from 0.41303 to 0.41357, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 14s 40ms/step - loss: 1.0934 - accuracy: 0.3982 - val_loss: 1.0851 - val_accuracy: 0.4136\n",
      "Epoch 3/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 1.0858 - accuracy: 0.4105\n",
      "Epoch 3: val_accuracy improved from 0.41357 to 0.42122, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 16s 45ms/step - loss: 1.0859 - accuracy: 0.4103 - val_loss: 1.0779 - val_accuracy: 0.4212\n",
      "Epoch 4/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 1.0766 - accuracy: 0.4210\n",
      "Epoch 4: val_accuracy improved from 0.42122 to 0.45797, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 15s 42ms/step - loss: 1.0766 - accuracy: 0.4210 - val_loss: 1.0642 - val_accuracy: 0.4580\n",
      "Epoch 5/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 1.0459 - accuracy: 0.4645\n",
      "Epoch 5: val_accuracy improved from 0.45797 to 0.51510, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 15s 44ms/step - loss: 1.0458 - accuracy: 0.4647 - val_loss: 1.0095 - val_accuracy: 0.5151\n",
      "Epoch 6/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.9597 - accuracy: 0.5514\n",
      "Epoch 6: val_accuracy improved from 0.51510 to 0.60826, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 15s 44ms/step - loss: 0.9595 - accuracy: 0.5516 - val_loss: 0.9010 - val_accuracy: 0.6083\n",
      "Epoch 7/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.8571 - accuracy: 0.6261\n",
      "Epoch 7: val_accuracy improved from 0.60826 to 0.64156, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 15s 44ms/step - loss: 0.8574 - accuracy: 0.6259 - val_loss: 0.8317 - val_accuracy: 0.6416\n",
      "Epoch 8/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.6752\n",
      "Epoch 8: val_accuracy improved from 0.64156 to 0.66576, saving model to best_model0.hdf5\n",
      "344/344 [==============================] - 15s 44ms/step - loss: 0.7850 - accuracy: 0.6752 - val_loss: 0.8008 - val_accuracy: 0.6658\n"
     ]
    }
   ],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(layers.Embedding(max_words, 40, input_length=max_len))\n",
    "model0.add(layers.SimpleRNN(125, kernel_regularizer=regularizers.l2(l=0.0001)))\n",
    "model0.add(layers.Dropout(0.8))\n",
    "model0.add(layers.Dense(10, activation='relu'))\n",
    "model0.add(layers.Dense(3,activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model0.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model0.summary()\n",
    "\n",
    "checkpoint0 = ModelCheckpoint(\"best_model0.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model0.fit(X_train, y_train, epochs=8, batch_size=64, validation_data=(X_test, y_test),callbacks=[checkpoint0])\n",
    "model0.save('RNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221ed6e",
   "metadata": {},
   "source": [
    "### b. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1e202cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_24 (Embedding)    (None, 200, 40)           200000    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 20)                4880      \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 3)                 63        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,943\n",
      "Trainable params: 204,943\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 1.0219 - accuracy: 0.4636\n",
      "Epoch 1: val_accuracy improved from -inf to 0.62373, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 16s 43ms/step - loss: 1.0221 - accuracy: 0.4636 - val_loss: 0.8537 - val_accuracy: 0.6237\n",
      "Epoch 2/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.8598 - accuracy: 0.6082\n",
      "Epoch 2: val_accuracy improved from 0.62373 to 0.67704, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 16s 48ms/step - loss: 0.8598 - accuracy: 0.6082 - val_loss: 0.7626 - val_accuracy: 0.6770\n",
      "Epoch 3/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.8078 - accuracy: 0.6441\n",
      "Epoch 3: val_accuracy improved from 0.67704 to 0.68977, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 18s 52ms/step - loss: 0.8080 - accuracy: 0.6439 - val_loss: 0.7433 - val_accuracy: 0.6898\n",
      "Epoch 4/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.7722 - accuracy: 0.6697\n",
      "Epoch 4: val_accuracy improved from 0.68977 to 0.69687, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 18s 51ms/step - loss: 0.7722 - accuracy: 0.6697 - val_loss: 0.7238 - val_accuracy: 0.6969\n",
      "Epoch 5/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.7480 - accuracy: 0.6824\n",
      "Epoch 5: val_accuracy improved from 0.69687 to 0.70015, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 20s 57ms/step - loss: 0.7480 - accuracy: 0.6824 - val_loss: 0.7135 - val_accuracy: 0.7001\n",
      "Epoch 6/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.7328 - accuracy: 0.6888\n",
      "Epoch 6: val_accuracy improved from 0.70015 to 0.70742, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 20s 57ms/step - loss: 0.7328 - accuracy: 0.6888 - val_loss: 0.7018 - val_accuracy: 0.7074\n",
      "Epoch 7/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.7216 - accuracy: 0.6974\n",
      "Epoch 7: val_accuracy improved from 0.70742 to 0.71161, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 19s 55ms/step - loss: 0.7216 - accuracy: 0.6974 - val_loss: 0.7008 - val_accuracy: 0.7116\n",
      "Epoch 8/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.7050 - accuracy: 0.7074\n",
      "Epoch 8: val_accuracy improved from 0.71161 to 0.71361, saving model to best_model3.hdf5\n",
      "344/344 [==============================] - 19s 55ms/step - loss: 0.7051 - accuracy: 0.7073 - val_loss: 0.7004 - val_accuracy: 0.7136\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(layers.Embedding(max_words, 40, input_length=max_len)) \n",
    "model1.add(layers.LSTM(20,dropout=0.9)) \n",
    "model1.add(layers.Dense(3,activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model1.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "checkpoint3 = ModelCheckpoint(\"best_model3.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model1.fit(X_train, y_train, epochs=8, batch_size=64, validation_data=(X_test, y_test),callbacks=[checkpoint3])\n",
    "model1.save('LSTM.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1698c",
   "metadata": {},
   "source": [
    "### c. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1eeaaa88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_25 (Embedding)    (None, 200, 40)           200000    \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 10)                1560      \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 10)                110       \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201,703\n",
      "Trainable params: 201,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 1.0695 - accuracy: 0.4272\n",
      "Epoch 1: val_accuracy improved from -inf to 0.52893, saving model to best_model2.hdf5\n",
      "344/344 [==============================] - 16s 41ms/step - loss: 1.0694 - accuracy: 0.4273 - val_loss: 0.9879 - val_accuracy: 0.5289\n",
      "Epoch 2/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.9785 - accuracy: 0.5077\n",
      "Epoch 2: val_accuracy improved from 0.52893 to 0.59825, saving model to best_model2.hdf5\n",
      "344/344 [==============================] - 15s 42ms/step - loss: 0.9783 - accuracy: 0.5079 - val_loss: 0.8765 - val_accuracy: 0.5983\n",
      "Epoch 3/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.9185 - accuracy: 0.5455\n",
      "Epoch 3: val_accuracy improved from 0.59825 to 0.65557, saving model to best_model2.hdf5\n",
      "344/344 [==============================] - 16s 48ms/step - loss: 0.9187 - accuracy: 0.5455 - val_loss: 0.8283 - val_accuracy: 0.6556\n",
      "Epoch 4/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.8824 - accuracy: 0.5776\n",
      "Epoch 4: val_accuracy improved from 0.65557 to 0.67849, saving model to best_model2.hdf5\n",
      "344/344 [==============================] - 16s 47ms/step - loss: 0.8824 - accuracy: 0.5776 - val_loss: 0.8151 - val_accuracy: 0.6785\n",
      "Epoch 5/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.8686 - accuracy: 0.5845\n",
      "Epoch 5: val_accuracy did not improve from 0.67849\n",
      "344/344 [==============================] - 16s 48ms/step - loss: 0.8684 - accuracy: 0.5846 - val_loss: 0.8022 - val_accuracy: 0.6643\n",
      "Epoch 6/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.8523 - accuracy: 0.5984\n",
      "Epoch 6: val_accuracy did not improve from 0.67849\n",
      "344/344 [==============================] - 17s 48ms/step - loss: 0.8524 - accuracy: 0.5984 - val_loss: 0.8029 - val_accuracy: 0.6765\n",
      "Epoch 7/8\n",
      "343/344 [============================>.] - ETA: 0s - loss: 0.8344 - accuracy: 0.6128\n",
      "Epoch 7: val_accuracy did not improve from 0.67849\n",
      "344/344 [==============================] - 17s 48ms/step - loss: 0.8342 - accuracy: 0.6128 - val_loss: 0.8091 - val_accuracy: 0.6690\n",
      "Epoch 8/8\n",
      "344/344 [==============================] - ETA: 0s - loss: 0.8333 - accuracy: 0.6169\n",
      "Epoch 8: val_accuracy did not improve from 0.67849\n",
      "344/344 [==============================] - 17s 48ms/step - loss: 0.8333 - accuracy: 0.6169 - val_loss: 0.8126 - val_accuracy: 0.6403\n"
     ]
    }
   ],
   "source": [
    "model2 = keras.Sequential()\n",
    "model2.add(layers.Embedding(max_words, 40, input_length=max_len)) \n",
    "model2.add(layers.GRU(10, kernel_regularizer=regularizers.l2(l=0.00001)))\n",
    "model2.add(layers.Dropout(0.9))\n",
    "model2.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(l=0.00001)))\n",
    "model2.add(layers.Dense(3, activation='softmax'))\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model2.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=8, batch_size=64, validation_data=(X_test, y_test),callbacks=[checkpoint2])\n",
    "model2.save('GRU.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e00fd6",
   "metadata": {},
   "source": [
    "### d. Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5685fd",
   "metadata": {},
   "source": [
    "i. Function to use the base-learners to predict the probabilities individually and stack the predictions made by the deep learning base-learners before flattening to a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d19a45db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    itr = 0\n",
    "    for model in members:\n",
    "        # predict the outputs using baselearners\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ef8eb",
   "metadata": {},
   "source": [
    "ii. Function to train the SVM meta-learner with the new inputs which are the stacked predictions from the base-learners together with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92a50e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stacked_model_svm(members, inputX, inputy):\n",
    "    # create dataset that contains the predicted probabilities from the baselearners\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit meta-learner\n",
    "    model = SVC(kernel='poly', C=0.00025)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "\n",
    "members = []\n",
    "model = load_model('RNN.h5')\n",
    "members.append(model)\n",
    "model = load_model('LSTM.h5')\n",
    "members.append(model)\n",
    "model = load_model('GRU.h5')\n",
    "members.append(model)\n",
    "\n",
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "# fit stacked model\n",
    "modelSVM = fit_stacked_model_svm(members, X_train, y_train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90822123",
   "metadata": {},
   "source": [
    "iii. Function to train the LR meta-learner with the new inputs which are the stacked predictions from the base-learners together with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b891dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stacked_model_lr(members, inputX, inputy):\n",
    "    # create dataset that contains the predicted probabilities from the baselearners\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit meta-learner\n",
    "    model = LogisticRegression(C=0.001, random_state=0)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "\n",
    "members = []\n",
    "model = load_model('RNN.h5')\n",
    "members.append(model)\n",
    "model = load_model('LSTM.h5')\n",
    "members.append(model)\n",
    "model = load_model('GRU.h5')\n",
    "members.append(model)\n",
    "\n",
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "# fit stacked model\n",
    "modelLR = fit_stacked_model_lr(members, X_train, y_train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd53c56",
   "metadata": {},
   "source": [
    "iv. Function to predict the output of data after training the meta-learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bef0e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset that contains the predicted probabilities from the baselearners\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # predict the outputs using ensemble model\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0f3ca0",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "Each of the models are evaluated using accuracy, precision, recall and F1-score which are obtained from the classification report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a51f9b",
   "metadata": {},
   "source": [
    "### a. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5380eac5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RNN MODEL \n",
      "===========\n",
      " TRAIN ACCURACY \n",
      "================\n",
      "687/687 [==============================] - 5s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.53      0.64      6258\n",
      "     neutral       0.63      0.81      0.71      8842\n",
      "    positive       0.81      0.76      0.78      6884\n",
      "\n",
      "    accuracy                           0.72     21984\n",
      "   macro avg       0.75      0.70      0.71     21984\n",
      "weighted avg       0.73      0.72      0.71     21984\n",
      "\n",
      " VALIDATION ACCURACY \n",
      "=====================\n",
      "172/172 [==============================] - 1s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.46      0.57      1523\n",
      "     neutral       0.60      0.76      0.67      2275\n",
      "    positive       0.73      0.71      0.72      1698\n",
      "\n",
      "    accuracy                           0.66      5496\n",
      "   macro avg       0.69      0.64      0.65      5496\n",
      "weighted avg       0.68      0.66      0.66      5496\n",
      "\n",
      " REVIEW TEST ACCURACY \n",
      "======================\n",
      "['anyone iphone pro iphone', 'remember first time buy smart phone iphone show grandfather proud could first question able make rich try prove right time crypto', 'iphone pro max take use new iphone notice interesting change use also many similarity iphone pro max user new feature change come across quite subtle', 'start save next iphone whatever pro', 'mf iphone fuck update everything become slow even pay apps rather switch anaroid stupid io']\n",
      "10/10 [==============================] - 0s 10ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.35      0.46       126\n",
      "     neutral       0.31      0.77      0.45        77\n",
      "    positive       0.70      0.36      0.48       107\n",
      "\n",
      "    accuracy                           0.46       310\n",
      "   macro avg       0.56      0.49      0.46       310\n",
      "weighted avg       0.59      0.46      0.46       310\n",
      "\n",
      " COVID TEST ACCURACY \n",
      "=====================\n",
      "['notice one wear mask everybody seem pretty good shape joe biden covid covid covidisnotover longcovid', 'dont positive time perfectly okay feel sad angry annoy frustrate scared anxious feeling doesnt make negative person make human lori deschene mentalhealth health anxiety depression covid support nft', 'legal happy million spend past year mourn death love one due covid last thing want need fuckimg sadness especially mandatory sadness rather watch spongebob monday notmyking', 'ccbq invite wellness wednesday september downtown brooklyn see fly information bethesolution resource service nyc covid coronavirus corona community love compassion socialgood joy philanthropy gratitude', 'covid vaccine booster omicron dont time clinical trial dont know degree vaccine wan yearly booster require happy natural immunity work adverse effect amp yearly booster cbc canada']\n",
      "10/10 [==============================] - 0s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      0.64      0.60       100\n",
      "     neutral       0.45      0.47      0.46       102\n",
      "    positive       0.69      0.57      0.62        88\n",
      "\n",
      "    accuracy                           0.56       290\n",
      "   macro avg       0.57      0.56      0.56       290\n",
      "weighted avg       0.57      0.56      0.56       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "\n",
    "model0 = load_model('RNN.h5')\n",
    "\n",
    "print(' RNN MODEL ')\n",
    "print('===========')\n",
    "\n",
    "print(' TRAIN ACCURACY ')\n",
    "print('================')\n",
    "y_predTrain = model0.predict(X_train)\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "predicted_train = []\n",
    "for y in y_predTrain:\n",
    "    predicted_train.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "    \n",
    "print(classification_report(y_train_transform, predicted_train))\n",
    "\n",
    "\n",
    "print(\" VALIDATION ACCURACY \")\n",
    "print(\"=====================\")\n",
    "y_predTest = model0.predict(X_test)\n",
    "y_test_transform = []\n",
    "for y in y_test:\n",
    "    y_test_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "predicted_test = []\n",
    "for y in y_predTest:\n",
    "    predicted_test.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "    \n",
    "print(classification_report(y_test_transform, predicted_test))\n",
    "\n",
    "\n",
    "print(\" REVIEW TEST ACCURACY \")\n",
    "print(\"======================\")\n",
    "reviewTest = pd.read_csv(\"iPhoneTest.csv\")\n",
    "X_reviewTest = reviewTest['text']\n",
    "y_reviewTest = reviewTest['sentiment'].values.tolist()\n",
    "\n",
    "X_reviewTest = X_reviewTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_reviewTest)):\n",
    "    temp.append(depure_data(X_reviewTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_reviewTest = tokenizer.texts_to_sequences(data)\n",
    "X_reviewTest = pad_sequences(X_reviewTest, maxlen=max_len)\n",
    "\n",
    "y_predictReview = model0.predict(X_reviewTest)\n",
    "predicted_sentiment_review = []\n",
    "for y in y_predictReview:\n",
    "    predicted_sentiment_review.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "print(classification_report(y_reviewTest, predicted_sentiment_review))\n",
    "\n",
    "print(\" COVID TEST ACCURACY \")\n",
    "print(\"=====================\")\n",
    "covidTest = pd.read_csv(\"covidTest.csv\")\n",
    "X_covidTest = covidTest['text']\n",
    "y_covidTest = covidTest['sentiment'].values.tolist()\n",
    "\n",
    "X_covidTest = X_covidTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_covidTest)):\n",
    "    temp.append(depure_data(X_covidTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_covidTest = tokenizer.texts_to_sequences(data)\n",
    "X_covidTest = pad_sequences(X_covidTest, maxlen=max_len)\n",
    "\n",
    "y_predictCovid = model0.predict(X_covidTest)\n",
    "predicted_sentiment_covid = []\n",
    "for y in y_predictCovid:\n",
    "    predicted_sentiment_covid.append(sentiment[np.around(y, decimals=0).argmax()]) \n",
    "print(classification_report(y_covidTest, predicted_sentiment_covid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b862df0",
   "metadata": {},
   "source": [
    "### b. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50afada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM MODEL \n",
      "============\n",
      " TRAIN ACCURACY \n",
      "================\n",
      "687/687 [==============================] - 6s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.68      0.74      6258\n",
      "     neutral       0.70      0.81      0.75      8842\n",
      "    positive       0.82      0.77      0.80      6884\n",
      "\n",
      "    accuracy                           0.76     21984\n",
      "   macro avg       0.78      0.76      0.76     21984\n",
      "weighted avg       0.77      0.76      0.76     21984\n",
      "\n",
      " VALIDATION ACCURACY \n",
      "=====================\n",
      "172/172 [==============================] - 2s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.61      0.68      1523\n",
      "     neutral       0.66      0.77      0.71      2275\n",
      "    positive       0.78      0.73      0.76      1698\n",
      "\n",
      "    accuracy                           0.72      5496\n",
      "   macro avg       0.73      0.71      0.71      5496\n",
      "weighted avg       0.72      0.72      0.71      5496\n",
      "\n",
      " REVIEW TEST ACCURACY \n",
      "======================\n",
      "['anyone iphone pro iphone', 'remember first time buy smart phone iphone show grandfather proud could first question able make rich try prove right time crypto', 'iphone pro max take use new iphone notice interesting change use also many similarity iphone pro max user new feature change come across quite subtle', 'start save next iphone whatever pro', 'mf iphone fuck update everything become slow even pay apps rather switch anaroid stupid io']\n",
      "10/10 [==============================] - 0s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.40      0.54       126\n",
      "     neutral       0.31      0.77      0.44        77\n",
      "    positive       0.75      0.38      0.51       107\n",
      "\n",
      "    accuracy                           0.49       310\n",
      "   macro avg       0.62      0.52      0.49       310\n",
      "weighted avg       0.66      0.49      0.50       310\n",
      "\n",
      " COVID TEST ACCURACY \n",
      "=====================\n",
      "['notice one wear mask everybody seem pretty good shape joe biden covid covid covidisnotover longcovid', 'dont positive time perfectly okay feel sad angry annoy frustrate scared anxious feeling doesnt make negative person make human lori deschene mentalhealth health anxiety depression covid support nft', 'legal happy million spend past year mourn death love one due covid last thing want need fuckimg sadness especially mandatory sadness rather watch spongebob monday notmyking', 'ccbq invite wellness wednesday september downtown brooklyn see fly information bethesolution resource service nyc covid coronavirus corona community love compassion socialgood joy philanthropy gratitude', 'covid vaccine booster omicron dont time clinical trial dont know degree vaccine wan yearly booster require happy natural immunity work adverse effect amp yearly booster cbc canada']\n",
      "10/10 [==============================] - 0s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.71      0.65       100\n",
      "     neutral       0.52      0.45      0.48       102\n",
      "    positive       0.73      0.67      0.70        88\n",
      "\n",
      "    accuracy                           0.61       290\n",
      "   macro avg       0.61      0.61      0.61       290\n",
      "weighted avg       0.61      0.61      0.60       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "\n",
    "model0 = load_model('LSTM.h5')\n",
    "\n",
    "print(' LSTM MODEL ')\n",
    "print('============')\n",
    "\n",
    "print(' TRAIN ACCURACY ')\n",
    "print('================')\n",
    "y_predTrain = model0.predict(X_train)\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "predicted_train = []\n",
    "for y in y_predTrain:\n",
    "    predicted_train.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "    \n",
    "print(classification_report(y_train_transform, predicted_train))\n",
    "\n",
    "\n",
    "print(\" VALIDATION ACCURACY \")\n",
    "print(\"=====================\")\n",
    "y_predTest = model0.predict(X_test)\n",
    "y_test_transform = []\n",
    "for y in y_test:\n",
    "    y_test_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "predicted_test = []\n",
    "for y in y_predTest:\n",
    "    predicted_test.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "    \n",
    "print(classification_report(y_test_transform, predicted_test))\n",
    "\n",
    "\n",
    "print(\" REVIEW TEST ACCURACY \")\n",
    "print(\"======================\")\n",
    "reviewTest = pd.read_csv(\"iPhoneTest.csv\")\n",
    "X_reviewTest = reviewTest['text']\n",
    "y_reviewTest = reviewTest['sentiment'].values.tolist()\n",
    "\n",
    "X_reviewTest = X_reviewTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_reviewTest)):\n",
    "    temp.append(depure_data(X_reviewTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_reviewTest = tokenizer.texts_to_sequences(data)\n",
    "X_reviewTest = pad_sequences(X_reviewTest, maxlen=max_len)\n",
    "\n",
    "y_predictReview = model0.predict(X_reviewTest)\n",
    "predicted_sentiment_review = []\n",
    "for y in y_predictReview:\n",
    "    predicted_sentiment_review.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "print(classification_report(y_reviewTest, predicted_sentiment_review))\n",
    "\n",
    "print(\" COVID TEST ACCURACY \")\n",
    "print(\"=====================\")\n",
    "covidTest = pd.read_csv(\"covidTest.csv\")\n",
    "X_covidTest = covidTest['text']\n",
    "y_covidTest = covidTest['sentiment'].values.tolist()\n",
    "\n",
    "X_covidTest = X_covidTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_covidTest)):\n",
    "    temp.append(depure_data(X_covidTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_covidTest = tokenizer.texts_to_sequences(data)\n",
    "X_covidTest = pad_sequences(X_covidTest, maxlen=max_len)\n",
    "\n",
    "y_predictCovid = model0.predict(X_covidTest)\n",
    "predicted_sentiment_covid = []\n",
    "for y in y_predictCovid:\n",
    "    predicted_sentiment_covid.append(sentiment[np.around(y, decimals=0).argmax()]) \n",
    "print(classification_report(y_covidTest, predicted_sentiment_covid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc093d",
   "metadata": {},
   "source": [
    "### c. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "389ffd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GRU MODEL \n",
      "===========\n",
      " TRAIN ACCURACY \n",
      "================\n",
      "687/687 [==============================] - 5s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.36      0.52      6258\n",
      "     neutral       0.60      0.92      0.73      8842\n",
      "    positive       0.89      0.77      0.83      6884\n",
      "\n",
      "    accuracy                           0.72     21984\n",
      "   macro avg       0.80      0.69      0.69     21984\n",
      "weighted avg       0.78      0.72      0.70     21984\n",
      "\n",
      " VALIDATION ACCURACY \n",
      "=====================\n",
      "172/172 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.32      0.47      1523\n",
      "     neutral       0.57      0.87      0.69      2275\n",
      "    positive       0.79      0.68      0.73      1698\n",
      "\n",
      "    accuracy                           0.66      5496\n",
      "   macro avg       0.74      0.62      0.63      5496\n",
      "weighted avg       0.72      0.66      0.64      5496\n",
      "\n",
      " REVIEW TEST ACCURACY \n",
      "======================\n",
      "['anyone iphone pro iphone', 'remember first time buy smart phone iphone show grandfather proud could first question able make rich try prove right time crypto', 'iphone pro max take use new iphone notice interesting change use also many similarity iphone pro max user new feature change come across quite subtle', 'start save next iphone whatever pro', 'mf iphone fuck update everything become slow even pay apps rather switch anaroid stupid io']\n",
      "10/10 [==============================] - 0s 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.10      0.17       126\n",
      "     neutral       0.27      0.88      0.42        77\n",
      "    positive       0.73      0.34      0.46       107\n",
      "\n",
      "    accuracy                           0.37       310\n",
      "   macro avg       0.64      0.44      0.35       310\n",
      "weighted avg       0.70      0.37      0.33       310\n",
      "\n",
      " COVID TEST ACCURACY \n",
      "=====================\n",
      "['notice one wear mask everybody seem pretty good shape joe biden covid covid covidisnotover longcovid', 'dont positive time perfectly okay feel sad angry annoy frustrate scared anxious feeling doesnt make negative person make human lori deschene mentalhealth health anxiety depression covid support nft', 'legal happy million spend past year mourn death love one due covid last thing want need fuckimg sadness especially mandatory sadness rather watch spongebob monday notmyking', 'ccbq invite wellness wednesday september downtown brooklyn see fly information bethesolution resource service nyc covid coronavirus corona community love compassion socialgood joy philanthropy gratitude', 'covid vaccine booster omicron dont time clinical trial dont know degree vaccine wan yearly booster require happy natural immunity work adverse effect amp yearly booster cbc canada']\n",
      "10/10 [==============================] - 0s 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.43      0.55       100\n",
      "     neutral       0.45      0.72      0.55       102\n",
      "    positive       0.68      0.55      0.60        88\n",
      "\n",
      "    accuracy                           0.57       290\n",
      "   macro avg       0.63      0.56      0.57       290\n",
      "weighted avg       0.62      0.57      0.57       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "\n",
    "model0 = load_model('GRU.h5')\n",
    "\n",
    "print(' GRU MODEL ')\n",
    "print('===========')\n",
    "\n",
    "print(' TRAIN ACCURACY ')\n",
    "print('================')\n",
    "y_predTrain = model0.predict(X_train)\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "predicted_train = []\n",
    "for y in y_predTrain:\n",
    "    predicted_train.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "    \n",
    "print(classification_report(y_train_transform, predicted_train))\n",
    "\n",
    "\n",
    "print(\" VALIDATION ACCURACY \")\n",
    "print(\"=====================\")\n",
    "y_predTest = model0.predict(X_test)\n",
    "y_test_transform = []\n",
    "for y in y_test:\n",
    "    y_test_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "predicted_test = []\n",
    "for y in y_predTest:\n",
    "    predicted_test.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "    \n",
    "print(classification_report(y_test_transform, predicted_test))\n",
    "\n",
    "\n",
    "print(\" REVIEW TEST ACCURACY \")\n",
    "print(\"======================\")\n",
    "reviewTest = pd.read_csv(\"iPhoneTest.csv\")\n",
    "X_reviewTest = reviewTest['text']\n",
    "y_reviewTest = reviewTest['sentiment'].values.tolist()\n",
    "\n",
    "X_reviewTest = X_reviewTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_reviewTest)):\n",
    "    temp.append(depure_data(X_reviewTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_reviewTest = tokenizer.texts_to_sequences(data)\n",
    "X_reviewTest = pad_sequences(X_reviewTest, maxlen=max_len)\n",
    "\n",
    "y_predictReview = model0.predict(X_reviewTest)\n",
    "predicted_sentiment_review = []\n",
    "for y in y_predictReview:\n",
    "    predicted_sentiment_review.append(sentiment[np.around(y, decimals=0).argmax()])\n",
    "print(classification_report(y_reviewTest, predicted_sentiment_review))\n",
    "\n",
    "print(\" COVID TEST ACCURACY \")\n",
    "print(\"=====================\")\n",
    "covidTest = pd.read_csv(\"covidTest.csv\")\n",
    "X_covidTest = covidTest['text']\n",
    "y_covidTest = covidTest['sentiment'].values.tolist()\n",
    "\n",
    "X_covidTest = X_covidTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_covidTest)):\n",
    "    temp.append(depure_data(X_covidTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_covidTest = tokenizer.texts_to_sequences(data)\n",
    "X_covidTest = pad_sequences(X_covidTest, maxlen=max_len)\n",
    "\n",
    "y_predictCovid = model0.predict(X_covidTest)\n",
    "predicted_sentiment_covid = []\n",
    "for y in y_predictCovid:\n",
    "    predicted_sentiment_covid.append(sentiment[np.around(y, decimals=0).argmax()]) \n",
    "print(classification_report(y_covidTest, predicted_sentiment_covid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930786f2",
   "metadata": {},
   "source": [
    "### d. Stacked RNN-LSTM-GRU with SVM meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a41a0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ENSEMBLE MODEL WITH SVM \n",
      "=========================\n",
      " TRAIN SET ACCURACY \n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.68      0.74      6258\n",
      "     neutral       0.71      0.82      0.76      8842\n",
      "    positive       0.84      0.81      0.82      6884\n",
      "\n",
      "    accuracy                           0.78     21984\n",
      "   macro avg       0.79      0.77      0.78     21984\n",
      "weighted avg       0.78      0.78      0.78     21984\n",
      "\n",
      " VALIDATION SET ACCURACY \n",
      "=========================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.60      0.67      1523\n",
      "     neutral       0.66      0.77      0.71      2275\n",
      "    positive       0.77      0.75      0.76      1698\n",
      "\n",
      "    accuracy                           0.71      5496\n",
      "   macro avg       0.73      0.71      0.71      5496\n",
      "weighted avg       0.72      0.71      0.71      5496\n",
      "\n",
      " REVIEW TEST ACCURACY \n",
      "======================\n",
      "['anyone iphone pro iphone', 'remember first time buy smart phone iphone show grandfather proud could first question able make rich try prove right time crypto', 'iphone pro max take use new iphone notice interesting change use also many similarity iphone pro max user new feature change come across quite subtle', 'start save next iphone whatever pro', 'mf iphone fuck update everything become slow even pay apps rather switch anaroid stupid io']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.41      0.54       126\n",
      "     neutral       0.31      0.75      0.44        77\n",
      "    positive       0.72      0.39      0.51       107\n",
      "\n",
      "    accuracy                           0.49       310\n",
      "   macro avg       0.61      0.52      0.50       310\n",
      "weighted avg       0.65      0.49      0.51       310\n",
      "\n",
      " COVID TEST ACCURACY \n",
      "=====================\n",
      "['notice one wear mask everybody seem pretty good shape joe biden covid covid covidisnotover longcovid', 'dont positive time perfectly okay feel sad angry annoy frustrate scared anxious feeling doesnt make negative person make human lori deschene mentalhealth health anxiety depression covid support nft', 'legal happy million spend past year mourn death love one due covid last thing want need fuckimg sadness especially mandatory sadness rather watch spongebob monday notmyking', 'ccbq invite wellness wednesday september downtown brooklyn see fly information bethesolution resource service nyc covid coronavirus corona community love compassion socialgood joy philanthropy gratitude', 'covid vaccine booster omicron dont time clinical trial dont know degree vaccine wan yearly booster require happy natural immunity work adverse effect amp yearly booster cbc canada']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.60      0.72      0.65       100\n",
      "     neutral       0.48      0.44      0.46       102\n",
      "    positive       0.69      0.60      0.64        88\n",
      "\n",
      "    accuracy                           0.59       290\n",
      "   macro avg       0.59      0.59      0.59       290\n",
      "weighted avg       0.59      0.59      0.58       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "\n",
    "print(' ENSEMBLE MODEL WITH SVM ')\n",
    "print('=========================')\n",
    "print(' TRAIN SET ACCURACY ')\n",
    "print('====================')\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "yhat = stacked_prediction(members, modelSVM, X_train)\n",
    "\n",
    "print(classification_report(y_train_transform, yhat))\n",
    "\n",
    "\n",
    "print(' VALIDATION SET ACCURACY ')\n",
    "print('=========================')\n",
    "\n",
    "y_test_transform = []\n",
    "for y in y_test:\n",
    "    y_test_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "yhat = stacked_prediction(members, modelSVM, X_test)\n",
    "\n",
    "print(classification_report(y_test_transform, yhat))\n",
    "\n",
    "\n",
    "print(\" REVIEW TEST ACCURACY \")\n",
    "print(\"======================\")\n",
    "reviewTest = pd.read_csv(\"iPhoneTest.csv\")\n",
    "X_reviewTest = reviewTest['text']\n",
    "y_reviewTest = reviewTest['sentiment'].values.tolist()\n",
    "\n",
    "X_reviewTest = X_reviewTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_reviewTest)):\n",
    "    temp.append(depure_data(X_reviewTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_reviewTest = tokenizer.texts_to_sequences(data)\n",
    "X_reviewTest = pad_sequences(X_reviewTest, maxlen=max_len)\n",
    "\n",
    "y_predictReview = stacked_prediction(members, modelSVM, X_reviewTest)\n",
    "\n",
    "print(classification_report(y_reviewTest, y_predictReview))\n",
    "\n",
    "\n",
    "print(\" COVID TEST ACCURACY \")\n",
    "print(\"=====================\")\n",
    "covidTest = pd.read_csv(\"covidTest.csv\")\n",
    "X_covidTest = covidTest['text']\n",
    "y_covidTest = covidTest['sentiment'].values.tolist()\n",
    "\n",
    "X_covidTest = X_covidTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_covidTest)):\n",
    "    temp.append(depure_data(X_covidTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_covidTest = tokenizer.texts_to_sequences(data)\n",
    "X_covidTest = pad_sequences(X_covidTest, maxlen=max_len)\n",
    "\n",
    "y_predictCovid = stacked_prediction(members, modelSVM, X_covidTest)\n",
    "\n",
    "print(classification_report(y_covidTest, y_predictCovid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35e4c1",
   "metadata": {},
   "source": [
    "### e. Stacked RNN-LSTM-GRU with LR meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cde9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ENSEMBLE MODEL WITH LR \n",
      "========================\n",
      " TRAIN SET ACCURACY \n",
      "====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.67      0.74      6258\n",
      "     neutral       0.70      0.81      0.76      8842\n",
      "    positive       0.83      0.80      0.82      6884\n",
      "\n",
      "    accuracy                           0.77     21984\n",
      "   macro avg       0.78      0.76      0.77     21984\n",
      "weighted avg       0.78      0.77      0.77     21984\n",
      "\n",
      " VALIDATION SET ACCURACY \n",
      "=========================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.60      0.67      1523\n",
      "     neutral       0.66      0.76      0.71      2275\n",
      "    positive       0.76      0.75      0.76      1698\n",
      "\n",
      "    accuracy                           0.71      5496\n",
      "   macro avg       0.73      0.70      0.71      5496\n",
      "weighted avg       0.72      0.71      0.71      5496\n",
      "\n",
      " REVIEW TEST ACCURACY \n",
      "======================\n",
      "['anyone iphone pro iphone', 'remember first time buy smart phone iphone show grandfather proud could first question able make rich try prove right time crypto', 'iphone pro max take use new iphone notice interesting change use also many similarity iphone pro max user new feature change come across quite subtle', 'start save next iphone whatever pro', 'mf iphone fuck update everything become slow even pay apps rather switch anaroid stupid io']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.40      0.53       126\n",
      "     neutral       0.31      0.78      0.44        77\n",
      "    positive       0.74      0.37      0.50       107\n",
      "\n",
      "    accuracy                           0.48       310\n",
      "   macro avg       0.62      0.52      0.49       310\n",
      "weighted avg       0.66      0.48      0.50       310\n",
      "\n",
      " COVID TEST ACCURACY \n",
      "=====================\n",
      "['notice one wear mask everybody seem pretty good shape joe biden covid covid covidisnotover longcovid', 'dont positive time perfectly okay feel sad angry annoy frustrate scared anxious feeling doesnt make negative person make human lori deschene mentalhealth health anxiety depression covid support nft', 'legal happy million spend past year mourn death love one due covid last thing want need fuckimg sadness especially mandatory sadness rather watch spongebob monday notmyking', 'ccbq invite wellness wednesday september downtown brooklyn see fly information bethesolution resource service nyc covid coronavirus corona community love compassion socialgood joy philanthropy gratitude', 'covid vaccine booster omicron dont time clinical trial dont know degree vaccine wan yearly booster require happy natural immunity work adverse effect amp yearly booster cbc canada']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.71      0.64       100\n",
      "     neutral       0.50      0.45      0.47       102\n",
      "    positive       0.71      0.61      0.66        88\n",
      "\n",
      "    accuracy                           0.59       290\n",
      "   macro avg       0.60      0.59      0.59       290\n",
      "weighted avg       0.59      0.59      0.59       290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = ['neutral', 'positive', 'negative']\n",
    "\n",
    "print(' ENSEMBLE MODEL WITH LR ')\n",
    "print('========================')\n",
    "print(' TRAIN SET ACCURACY ')\n",
    "print('====================')\n",
    "y_train_transform = []\n",
    "for y in y_train:\n",
    "    y_train_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "yhat = stacked_prediction(members, modelLR, X_train)\n",
    "\n",
    "print(classification_report(y_train_transform, yhat))\n",
    "\n",
    "\n",
    "print(' VALIDATION SET ACCURACY ')\n",
    "print('=========================')\n",
    "y_test_transform = []\n",
    "for y in y_test:\n",
    "    y_test_transform.append(sentiment[np.around(y, decimals = 0).argmax()])\n",
    "\n",
    "yhat = stacked_prediction(members, modelLR, X_test)\n",
    "\n",
    "print(classification_report(y_test_transform, yhat))\n",
    "\n",
    "\n",
    "print(\" REVIEW TEST ACCURACY \")\n",
    "print(\"======================\")\n",
    "reviewTest = pd.read_csv(\"iPhoneTest.csv\")\n",
    "X_reviewTest = reviewTest['text']\n",
    "y_reviewTest = reviewTest['sentiment'].values.tolist()\n",
    "\n",
    "X_reviewTest = X_reviewTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_reviewTest)):\n",
    "    temp.append(depure_data(X_reviewTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_reviewTest = tokenizer.texts_to_sequences(data)\n",
    "X_reviewTest = pad_sequences(X_reviewTest, maxlen=max_len)\n",
    "\n",
    "y_predictReview = stacked_prediction(members, modelLR, X_reviewTest)\n",
    "\n",
    "print(classification_report(y_reviewTest, y_predictReview))\n",
    "\n",
    "\n",
    "print(\" COVID TEST ACCURACY \")\n",
    "print(\"=====================\")\n",
    "covidTest = pd.read_csv(\"covidTest.csv\")\n",
    "X_covidTest = covidTest['text']\n",
    "y_covidTest = covidTest['sentiment'].values.tolist()\n",
    "\n",
    "X_covidTest = X_covidTest.values.tolist()\n",
    "temp = []\n",
    "for i in range(len(X_covidTest)):\n",
    "    temp.append(depure_data(X_covidTest[i]))\n",
    "data_words = list(sent_to_words(temp))\n",
    "detokenized = []\n",
    "for i in range(len(data_words)):\n",
    "    detokenized.append(detokenize(data_words[i]))\n",
    "data = []\n",
    "for i in range(len(detokenized)):\n",
    "    data.append(stopwords_lemmatize(detokenized[i]))\n",
    "print(data[:5])\n",
    "X_covidTest = tokenizer.texts_to_sequences(data)\n",
    "X_covidTest = pad_sequences(X_covidTest, maxlen=max_len)\n",
    "\n",
    "y_predictCovid = stacked_prediction(members, modelLR, X_covidTest)\n",
    "\n",
    "print(classification_report(y_covidTest, y_predictCovid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74bae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
